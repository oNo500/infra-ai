---
title: 跨请求 LRU 缓存
impact: HIGH
impactDescription: caches across requests
tags: 'server, cache, lru, cross-request'
---

## 跨请求 LRU 缓存

`React.cache()` 仅在一个请求内工作。对于跨连续请求共享的数据（用户点击按钮 A，然后点击按钮 B），使用 LRU 缓存。

**Implementation:**

```typescript
import { LRUCache } from 'lru-cache'

const cache = new LRUCache<string, any>({
  max: 1000,
  ttl: 5 * 60 * 1000  // 5 minutes
})

export async function getUser(id: string) {
  const cached = cache.get(id)
  if (cached) return cached

  const user = await db.user.findUnique({ where: { id } })
  cache.set(id, user)
  return user
}

// Request 1: DB 查询, 结果缓存
// Request 2: 缓存命中, 无 DB 查询
```

当连续的用户操作在几秒钟内涉及到需要相同数据的多个端点时使用。

**使用 Vercel 的 [Fluid Compute](https://vercel.com/docs/fluid-compute):** LRU 缓存特别有效，因为多个并发请求可以共享相同的函数实例和缓存。这意味着缓存在请求之间持久存在，而无需像 Redis 这样的外部存储。

**在传统的 serverless 中:** 每次调用都在隔离中运行，因此考虑使用 Redis 进行跨进程缓存。

Reference: [https://github.com/isaacs/node-lru-cache](https://github.com/isaacs/node-lru-cache)
